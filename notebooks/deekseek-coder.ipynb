{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "866450a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaushal/mystuff/thinklang/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4893399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = os.getcwd() + \"/cache/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da65a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d34f9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/kaushal/mystuff/thinklang/examples'\n",
    "\n",
    "def gather_initial_prompt():\n",
    "    training_codes = \"\"\n",
    "    sample_codes = [f\"{path}/{f}\" for f in os.listdir(path=path)]\n",
    "\n",
    "    # map them into a single string and feed that into the model as context\n",
    "    for code_loc in sample_codes:\n",
    "        with open(code_loc) as f:\n",
    "            training_codes += f.read()\n",
    "            training_codes += \"\\n\"\n",
    "\n",
    "    return training_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a simple FizzBuzz program in the specified language:\n",
      "\n",
      "```python\n",
      "for i in range(1, 101):\n",
      "    if i % 3 == 0 and i % 5 == 0:\n",
      "        print(\"FizzBuzz\")\n",
      "    elif i % 3 == 0:\n",
      "        print(\"Fizz\")\n",
      "    elif i % 5 == 0:\n",
      "        print(\"Buzz\")\n",
      "    else:\n",
      "        print(i)\n",
      "```\n",
      "\n",
      "This program uses a for loop to iterate over the numbers 1 to 100. For each number, it checks if the number is divisible by 3 and 5, if so, it prints \"FizzBuzz\". If not, it checks if the number is divisible by 3, if so, it prints \"Fizz\", if the number is divisible by 5, it prints \"Buzz\", and if the number is not divisible by either, it prints the number itself.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "messages=[\n",
    "    { 'role': 'user', 'content': f\"\"\"You are a software developer writing code in a new language. The language syntax is shown below:\n",
    "        context: {gather_initial_prompt()}\n",
    "\n",
    "        You are to code in this langauge using this syntax. Write just the source code without any explanation.\n",
    "        \n",
    "        Prompt: Write a fizzbuzz program using the syntax specified in the context.\"\"\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "# tokenizer.eos_token_id is the id of <|EOT|> token\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83d678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
